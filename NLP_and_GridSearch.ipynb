{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will demonstrate how data was cleaned (tokenized, lemmatized, etc) and how the best model is chosen. The models tested here are Logistic Regression and Multinomial Naive Bayes, along with testing to compare performance between using CountVectorizer and TFIDFVectorizer on this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Cleaning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many reddit posts can have special characters and emojis, along with some links. These are two very text-heavy subreddits, so there should not be much pollution by external links or images, however to remove links that may be present I will target strings that contain \"www.\" or \"https:\" and remove only those elements, but keep the rest of the string if anything else remains. The data will also be lemmatized at this stage. \"Cleaning\" is in quotes here because some of the modeling below will involve either including or excluding stopwords, which could be considered another form of cleaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/combined_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with \" \"\n",
    "# not filling with symbol (since it will be scrubbed later), or word since this could pollute the data \n",
    "df.fillna(\" \", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, get rid of links\n",
    "# target text that has \"www.\" or \"https:\"\n",
    "\n",
    "# scrubbing links from titles\n",
    "\n",
    "for title in df[\"title\"]: \n",
    "    # convert title to lowercase \n",
    "    lower_title = title.lower()\n",
    "    \n",
    "    # split title into individual words\n",
    "    title_tokens = lower_title.split()\n",
    "    \n",
    "    # loop through each title searching for links, remove links ONLY\n",
    "    for token in title_tokens: \n",
    "        if \"www.\" in token: \n",
    "            title_tokens.remove(token)\n",
    "        elif \"http://\" in token: \n",
    "            title_tokens.remove(token)\n",
    "        elif \"https://\" in token: \n",
    "            title_tokens.remove(token)\n",
    "            \n",
    "    # combined processed words back into one string\n",
    "    processed_title = \" \".join(title_tokens)\n",
    "    \n",
    "    # replace old title with processed title\n",
    "    df[\"title\"].replace(to_replace = title, value = processed_title, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing is done for body text of posts \n",
    "\n",
    "for body in df[\"selftext\"]: \n",
    "    \n",
    "    lower_body = body.lower()\n",
    "    body_tokens = lower_body.split()\n",
    "    \n",
    "    for token in body_tokens: \n",
    "        if \"www.\" in token: \n",
    "            body_tokens.remove(token)\n",
    "        elif \"http://\" in token: \n",
    "            body_tokens.remove(token)\n",
    "        elif \"https://\" in token: \n",
    "            body_tokens.remove(token)\n",
    "            \n",
    "    processed_body = \" \".join(body_tokens)\n",
    "    df[\"selftext\"].replace(to_replace = body, value = processed_body, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, remove special (in this case, non-letter) characters \n",
    "# combine this step with lemmatizing \n",
    "\n",
    "tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for body in df[\"selftext\"]: \n",
    "    words = tokenizer.tokenize(body)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    processed = \" \".join(lemmatized)\n",
    "    df[\"selftext\"].replace(to_replace = body, value = processed, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in df[\"title\"]: \n",
    "    words = tokenizer.tokenize(title)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in words]\n",
    "    processed = \" \".join(lemmatized)\n",
    "    df[\"title\"].replace(to_replace = title, value = processed, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some brainstorming with our Merciful Overloard Charlie, combining the text from both the title and selftext into one column seemed like a great idea to organize the text for modeling. I don't really want to go back and do it from the beginning and then have to re-write cleaning for the one column right now, so I'm just going to create a new column using the two cleaned columns. I'll fix this when I have some more time for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"title\"] + \" \" + df[\"selftext\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2069, 14466)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of curiosity let's CountVectorize the text column \n",
    "\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "curiosity = cvec.fit_transform(df[\"text\"])\n",
    "\n",
    "curiosity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holy moly, 14,466 distinct words and that is without including word combinations! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.535524\n",
       "1    0.464476\n",
       "Name: abusive_relationship, dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"abusive_relationship\"].value_counts(normalize = True)\n",
    "\n",
    "# close to equal distribution of abusive relationship occurrences, no need to stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"text\"]\n",
    "\n",
    "y = df[\"abusive_relationship\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section explores the best way to model this data using either CountVectorizer or TFIDFVectorizer in combination with either a Logistic Regression model or a Multinomial Naive Bayes model. I use GridSearchCV to test a range of values for hyperparameters for each model to determine the best configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_list = list(np.arange(0.1, 0.3, 0.1))\n",
    "\n",
    "max_list = list(np.arange(0.5, 1.0, 0.2))\n",
    "\n",
    "vec_params = {\n",
    "    \"vec__ngram_range\": [(1, 1), (1, 2), (1, 3)], \n",
    "    \"vec__stop_words\": [None, \"english\"], \n",
    "    \"vec__max_features\": [300, 500, 700, 900], \n",
    "#     \"vec__min_df\": [min_list], \n",
    "#     \"vec__max_df\": [max_list]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tfidf_log = Pipeline([\n",
    "    (\"vec\", TfidfVectorizer()), \n",
    "    (\"lr\", LogisticRegression(solver = \"liblinear\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe_tfidf_log, vec_params, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vec',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=...\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='liblinear',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'vec__max_features': [300, 500, 700, 900],\n",
       "                         'vec__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
       "                         'vec__stop_words': [None, 'english']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vec__max_features': 300,\n",
       " 'vec__ngram_range': (1, 2),\n",
       " 'vec__stop_words': 'english'}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9150552486187845"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1448, 1)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1448,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
